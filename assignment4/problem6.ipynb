{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Language as Vectors with Word2Vec and Cosine Similarity\n",
    "\n",
    "For this problem, you'll use a popular technique called Word2Vec that represents words as vectors in a high-dimensional space. The key idea is that words that are used in similar ways will be close to each other in this space. You can read the original paper by Mikolov et al. [here](https://arxiv.org/pdf/1301.3781.pdf).\n",
    "\n",
    "## How it works\n",
    "\n",
    "Word2Vec uses a shallow neural network to learn word associations from a large corpus of text. It has two main architectures for training: Continuous Bag of Words (CBOW) and Skip-Gram.\n",
    "\n",
    "- **Continuous Bag of Words (CBOW):** This model predicts a target word based on its surrounding context words. For example, given the sentence \"the cat sat on the mat,\" CBOW would use the words \"the,\" \"sat,\" \"on,\" and \"the\" to predict \"cat\".\n",
    "- **Skip-Gram:** This model works in the reverse manner of CBOW. It uses a target word to predict its surrounding context words. Using the same sentence, Skip-Gram would take \"cat\" as input to predict \"the,\" \"sat,\" \"on,\" and \"the\".\n",
    "\n",
    "Both models rely on the distributional hypothesis, which suggests that words appearing in similar contexts tend to have similar meanings. Through training, Word2Vec adjusts the vectors so that words with similar meanings are positioned close to each other in the vector space.\n",
    "\n",
    "## Using it for language comparison\n",
    "\n",
    "Of course, we don't need to know the exact details of how it works. We can use pre-trained Word2Vec models to compare languages. In this notebook, we'll use the `gensim` library to load a pre-trained Word2Vec model and compare the similarity of different words. The application for linear algebra is to experiment with cosine similarity, which measures the angle between two vectors. In this case, the result of cosine similarity will give us a measure of how similar two words are in meaning.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, you'll need to make sure you have all the necessary libraries installed. On canvas, you can find a file called `cse3380a4.yml` that lists all the libraries you'll need. You can install them by running the following command:\n",
    "\n",
    "```bash\n",
    "conda env create -f cse3380a4.yml\n",
    "```\n",
    "\n",
    "If that doesn't work for you, you can find a list of the libraries you'll need below:\n",
    "\n",
    "- `gensim`\n",
    "- `numpy`\n",
    "- `matplotlib`\n",
    "- `scipy==1.12.0`\n",
    "\n",
    "To use the model, you'll need to download the pre-trained weights. You can find them [here](https://drive.google.com/file/d/19Z7UreEhC8Gm-PoWCWxXWSF0pFqgCUno/view?usp=sharing). Make sure you remember where you downloaded the file, as you'll need to provide the path to the model in the code below.\n",
    "\n",
    "If everything is downloaded and installed, you should be able to run the cell below. Note that this may take a few minutes to load the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "model_file = 'GoogleNews-vectors-negative300.bin.gz' # path to the model file\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(model_file, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the model\n",
    "\n",
    "You can retrieve words from the model along with their embedded vectors. To do so, you can use the `key_to_index` attribute of the model. This will give you a dictionary where the keys are the words and the values are the corresponding vectors. An example is provided in the cell below.\n",
    "\n",
    "The embedding is a 300-dimensional vector for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vehicle [ 0.33789062 -0.05249023  0.08496094 -0.18847656 -0.05688477 -0.14355469\n",
      "  0.16308594 -0.17285156  0.26367188 -0.12402344  0.16699219 -0.13867188\n",
      " -0.0135498  -0.0177002   0.03930664 -0.05712891 -0.15820312 -0.11572266\n",
      "  0.15820312 -0.09667969  0.12695312 -0.30273438 -0.2421875   0.04638672\n",
      "  0.11474609 -0.03515625 -0.04003906  0.32617188  0.32421875 -0.07275391\n",
      " -0.19628906 -0.25976562 -0.03393555  0.06689453 -0.10400391 -0.11914062\n",
      "  0.15136719 -0.05249023 -0.16601562 -0.04418945 -0.03198242  0.11181641\n",
      " -0.07324219  0.24414062 -0.32226562  0.01141357  0.0189209   0.1640625\n",
      "  0.265625    0.08642578 -0.31054688  0.16699219  0.16992188  0.15136719\n",
      " -0.13378906  0.07910156  0.07080078  0.21582031  0.1328125  -0.10400391\n",
      "  0.08105469 -0.04516602 -0.35546875 -0.05639648 -0.13378906  0.27539062\n",
      " -0.02844238 -0.04907227 -0.03588867 -0.03125     0.01123047 -0.15917969\n",
      " -0.16601562  0.06445312 -0.05834961  0.05004883 -0.09960938 -0.08789062\n",
      " -0.00292969 -0.13378906  0.375       0.00063324 -0.15136719  0.22363281\n",
      " -0.02758789 -0.17578125 -0.22070312  0.11181641 -0.20117188 -0.03173828\n",
      " -0.09033203 -0.10791016  0.18945312 -0.16503906  0.05664062 -0.17578125\n",
      "  0.00759888 -0.10693359  0.296875   -0.13671875  0.06591797 -0.13476562\n",
      "  0.11279297  0.14941406  0.23339844 -0.21289062 -0.0255127  -0.15332031\n",
      "  0.11523438  0.02087402 -0.22167969 -0.21289062 -0.14453125 -0.15527344\n",
      "  0.24511719 -0.109375    0.03393555 -0.04614258 -0.23730469  0.11425781\n",
      "  0.16503906  0.09912109 -0.01965332  0.03198242  0.02539062  0.1640625\n",
      " -0.02905273  0.03271484  0.07666016  0.3125      0.0222168  -0.08105469\n",
      " -0.20898438 -0.12792969  0.18652344 -0.02868652  0.1640625  -0.41015625\n",
      " -0.06103516  0.22460938 -0.19433594 -0.04345703 -0.23535156  0.19628906\n",
      " -0.10058594  0.02514648 -0.02038574 -0.12451172 -0.09375    -0.13476562\n",
      "  0.25390625 -0.10693359  0.10498047 -0.00567627 -0.06640625  0.02282715\n",
      " -0.12255859  0.04638672 -0.11914062  0.0022583  -0.06445312  0.36328125\n",
      "  0.17675781 -0.15820312 -0.05834961 -0.20117188  0.13867188 -0.11425781\n",
      "  0.11083984 -0.17578125 -0.02331543 -0.06298828 -0.0035553  -0.08203125\n",
      " -0.10107422 -0.22558594  0.10595703 -0.18554688 -0.53515625  0.08789062\n",
      " -0.23242188 -0.09179688 -0.19433594 -0.00543213  0.02880859 -0.12451172\n",
      " -0.04785156  0.3359375  -0.3125      0.11083984  0.04492188 -0.12988281\n",
      " -0.22167969  0.15234375  0.06787109  0.13476562 -0.08056641 -0.02685547\n",
      "  0.10986328  0.00698853  0.05737305 -0.21484375 -0.14355469 -0.05273438\n",
      " -0.09423828 -0.16113281 -0.0279541   0.29492188  0.19238281  0.07910156\n",
      "  0.26367188  0.06640625 -0.22949219  0.09130859 -0.29492188  0.10644531\n",
      " -0.04516602  0.23828125 -0.15429688 -0.03295898 -0.20410156 -0.27148438\n",
      "  0.28320312 -0.06640625  0.03979492  0.05615234  0.08447266  0.19238281\n",
      "  0.10302734 -0.04443359  0.09716797  0.03027344 -0.20019531 -0.15136719\n",
      " -0.13476562 -0.09960938 -0.19238281 -0.0456543  -0.00708008 -0.10107422\n",
      "  0.25195312 -0.08203125 -0.13378906 -0.31835938 -0.1484375  -0.09472656\n",
      "  0.17382812 -0.12060547  0.17285156 -0.12402344  0.09960938  0.06738281\n",
      " -0.14453125 -0.03344727 -0.10498047  0.00775146 -0.03344727  0.01733398\n",
      "  0.12207031 -0.359375    0.07275391 -0.40039062 -0.01611328 -0.0534668\n",
      "  0.1953125   0.27734375 -0.25585938 -0.03198242 -0.375       0.06494141\n",
      " -0.08544922 -0.21386719  0.00671387  0.29296875 -0.09716797 -0.09033203\n",
      "  0.03857422 -0.18457031 -0.07617188  0.1171875  -0.07421875  0.09667969\n",
      "  0.10986328 -0.3046875   0.00674438  0.05151367  0.125       0.04785156\n",
      "  0.13964844  0.10791016 -0.40039062  0.1484375  -0.01293945  0.04248047\n",
      " -0.02258301  0.00927734 -0.00891113  0.22753906 -0.03808594 -0.17675781]\n"
     ]
    }
   ],
   "source": [
    "# Retrieve a word and its embedding\n",
    "result = model.index_to_key[1024]\n",
    "embedding = model[result]\n",
    "print(result, embedding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Vectors with Cosine Similarity\n",
    "\n",
    "The cell below is where the primary work will be done. You'll need to compare the similarity of different words using cosine similarity. The formula for cosine similarity is given by:\n",
    "\n",
    "$$\n",
    "\\text{cosine\\_similarity}(\\mathbf{a}, \\mathbf{b}) = \\frac{\\mathbf{a} \\cdot \\mathbf{b}}{\\|\\mathbf{a}\\| \\|\\mathbf{b}\\| }\n",
    "$$\n",
    "\n",
    "where $\\mathbf{a}$ and $\\mathbf{b}$ are the vectors you want to compare. The dot product of the two vectors is divided by the product of their magnitudes.\n",
    "\n",
    "You can use `numpy` and the `linalg` module to compute the dot product and magnitudes of the vectors. You should complete the function `cosine_similarity` below to compute the cosine similarity between two vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cats', 0.809938), ('dog', 0.76094574), ('kitten', 0.7464985), ('feline', 0.73262346), ('beagle', 0.7150583), ('puppy', 0.70754534), ('pup', 0.6934291), ('pet', 0.68915313), ('felines', 0.6755932), ('chihuahua', 0.67097616)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def similarity_search(word, similarity_func, model, topn=10):\n",
    "    \"\"\"Compare similarity to each word in the model.\n",
    "\n",
    "    Args:\n",
    "        word (str): The word to compare similarity to.\n",
    "        similarity_func (function): The similarity function to use.\n",
    "        model (gensim.models.KeyedVectors): The model to use.\n",
    "        topn (int): The number of most similar words to return.\n",
    "\n",
    "    Returns:\n",
    "        list: The top n most similar words.\n",
    "    \"\"\"\n",
    "    # Create a dictionary that will store the similarities\n",
    "    # The keys are the words, the values are the similarities\n",
    "    similarities = {}\n",
    "\n",
    "    # Get the embedding for the search word\n",
    "    word_embedding = model[word]\n",
    "\n",
    "    # Compare the search word to each word in the model\n",
    "    for word2 in model.key_to_index:\n",
    "        word2_embedding = model[word2]\n",
    "\n",
    "        # Skip the word itself\n",
    "        if(word == word2): continue\n",
    "        # Calculate the similarity, storing it in the dictionary associated with the word\n",
    "        similarities[word2] = similarity_func(word_embedding, word2_embedding)\n",
    "\n",
    "    # Sort by similarity\n",
    "    most_similar = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Return the top n most similar words\n",
    "    return most_similar[:topn]\n",
    "\n",
    "\n",
    "def cosine_similarity(word1_vec, word2_vec):\n",
    "    \"\"\"Returns cosine similarity between two vectors.\"\"\"\n",
    "    # TODO: Implement cosine similarity\n",
    "    dot_product = np.dot(word1_vec, word2_vec)\n",
    "    normal_w1 = np.linalg.norm(word1_vec)\n",
    "    normal_w2 = np.linalg.norm(word2_vec)\n",
    "    return dot_product/(normal_w1*normal_w2)\n",
    "\n",
    "\n",
    "# Return the top 10 most similar words to \"king\"\n",
    "print(similarity_search(\"cat\", cosine_similarity, model))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with `gensim`'s similarity function\n",
    "\n",
    "The cell below will compare the cosine similarity of your input word using the `gensim` library. You can use this as a reference to check your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cats', 0.8099378347396851), ('dog', 0.7609457969665527), ('kitten', 0.7464984655380249), ('feline', 0.7326234579086304), ('beagle', 0.7150582671165466), ('puppy', 0.7075453996658325), ('pup', 0.6934291124343872), ('pet', 0.6891530752182007), ('felines', 0.6755931973457336), ('chihuahua', 0.6709762215614319)]\n"
     ]
    }
   ],
   "source": [
    "# Return the top 10 most similar words to \"king\"\n",
    "print(model.most_similar(\"cat\", topn=10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cse3380a4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
